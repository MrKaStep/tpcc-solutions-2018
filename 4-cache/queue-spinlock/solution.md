## Почему так делать хорошо?

Покажем, что реализованный спинлок промахивается по кешу только
константное число раз на каждую операцию. Посмотрим, при работе с какими ячейками может произойти
cache miss:

1. `spinlock_.wait_queue_tail_`. В этом случае всё просто: каждый поток производит одну запись и одно
чтение, поэтому и количество промахов будет константным.

1.  `next_`. В этом случае каждый поток производит одну запись в `AcquireLock`, что инвалидирует 
кэш-линии у константного числа потоков, ограниченного размером кэш-линии. 

1. `is_owner_`. Аналогично предыдущему пункту, но запись производится в `ReleaseLock`.

Таким образом, мы показали константное число промахов, но только амортизированно.

Теперь покажем, что нет проблем *ping-pong, thundering herd, false sharing*.

1. *ping-pong*. Заметим, что единственная ячейка, используемая всеми потоками - это 
`spinlock_.wait_queue_tail_`, которая изменяется и читается константное число раз для каждого потока,
поэтому с ней проблем не возникает. Остальные же ячейки являются локальными для каждого потока,
поэтому  

1. *thundering herd*. В данной реализации спинлока нет такого момента, когда потоки соревнуются за
владение локом, каждый поток просыпается когда его флаг `is_owner_` выставлен, не встречая 
конкуренции других потоков.

1. *false sharing*. Потоки могут разделять одну кэш-линию только в случае неудачного стечения
обстоятельств, но и в этом случае не происходит постоянного изменения ячеек, а только их чтение,
что не будет приводить к постоянной инвалидации кэш-линии

## Почему по-другому плохо?

Все три предложенных лока: `TestAndSet`, `TestAndTAS` и `Ticket` используют константное число ячеек
памяти, к которым обращаются все потоки. Так, при захвате лока, во всех трёх спинлоках происходит
запись в одну из общих ячеек, что инвалидирует кэш у все ожидающих потоков и, как следствие, приводит
к промахам по кэшу у всех ожидающих. То есть, каждый захват будет порождать линейное от числа потоков
количество промахов по кэшу.

    